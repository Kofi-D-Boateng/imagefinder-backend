package com.kboat.imagefinder.model.scraper;


import com.kboat.imagefinder.enums.CrawlMode;
import com.kboat.imagefinder.model.graph.ConcurrentWebGraphADT;
import com.kboat.imagefinder.model.graph.WebGraphADT;
import com.kboat.imagefinder.model.preprocessor.RegexProcessor;

import java.io.IOException;
import java.util.HashSet;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.ConcurrentHashMap;
import java.util.concurrent.CountDownLatch;
import java.util.concurrent.ExecutorService;
import java.util.concurrent.Executors;

/*
    A Multi-threading web scraper that will create a thread pool
    based on the number of pre-processed urls.
 */

public class Scraper implements Crawler {
    protected Set<String> urlSet;
    protected ConcurrentHashMap<String,ConcurrentHashMap<String,Set<String>>> imagesUrls;
    protected RegexProcessor processor;

    public Scraper(Set<String> urls, ConcurrentHashMap<String,ConcurrentHashMap<String,Set<String>>> imagesUrls, RegexProcessor stringProcessor) {
        this.urlSet = urls;
        this.imagesUrls = imagesUrls;
        this.processor = stringProcessor;
        createRootMap();
    }

    public Set<String> getUrls() {
        return urlSet;
    }

    public ConcurrentHashMap<String,ConcurrentHashMap<String,Set<String>>> getImagesFound() {
        return imagesUrls;
    }

    @Override
    public int itemsFound () {
        return this.imagesUrls.size();
    }

    @Override
    public void startCrawler(CrawlMode mode) throws IOException, InterruptedException {

        if(this.urlSet.size() > 1){
            System.out.println("[IN PROGRESS]:Starting multi-threaded crawler....");

             /*
                Create a pool based on the number of processed urls passed to the
                crawler.

                Creating a thread pool allows us to minimize the overhead of creating
                n-Threads, and allowing us to reuse resource.
            */
            ExecutorService pool = Executors.newFixedThreadPool(this.urlSet.size());
            /*
                In order to achieve synchronization between the spanning thread,
                We will use the CountDownLatch API. This API is similar to the
                Semaphore API however the CountDownLatch API is a one-time use
                which is what we need.

                We will:
                   Initialize the Latch with the url set size.
                   Start The crawler
                   Call the countdown method once all work is done.
            */
            CountDownLatch latch = new CountDownLatch(this.urlSet.size());
            /*
                Create a static array of tasks to be executed by the pool. Since we
                have to manually create multiple task and stop the pool execution,
                we will loop through the tasks array after filling it up.
            */

            ConcurrentWebGraphADT[] tasks = new ConcurrentWebGraphADT[this.urlSet.size()];
            /*
                Ranged looping over our root map will handle placing the task in the
                task queue.
            */
            int count = 0;
            for(Map.Entry<String,ConcurrentHashMap<String,Set<String>>> root:this.imagesUrls.entrySet()){
                ConcurrentWebGraphADT graphADT = new ConcurrentWebGraphADT(root.getKey(),root.getValue(),processor, mode);
                tasks[count++] = graphADT;
            }
            System.out.println("[COMPLETED]: Number of created tasks: " + tasks.length);

            /*
                Loops through all created ConcurrentWebGraph tasks and inserts findings
                into the concurrent hash map.
             */
            for(ConcurrentWebGraphADT task:tasks){
                pool.execute(()->{
                    task.run();

                    //ADD A CHECK TO SEE IF ANYTHING WAS FOUND BEFORE LOOPING OVER HASHMAP///

                    /*
                        TaskMap returns a mapping of tags (img/svg/etc) from the task.
                     */
                    Map<String, Set<String>> taskMap = task.getImageMap();
                    /*
                        TopLevelKey is the initial url processed from the response
                        generated by the end user and used to traverse through the
                        web page.
                     */
                    String topLevelKey = task.getUrl();
                    if(imagesUrls.containsKey(topLevelKey)){
                        /*
                            ImagesMap is a map from the topLevelKey (top domain)
                            that will have image tags map to their corresponding
                            image url.

                            We will range loop over the taskMap containing the
                            mapping of image tags to urls from the threaded task
                            and the keys and values into the imagesMap.

                         */
                        Map<String,Set<String>> imagesMap = imagesUrls.get(topLevelKey);
                        taskMap.forEach((tag,urlSet)->{
                            /*
                                If initial check is truthy, we will run a range loop
                                over the list from the task and insert all the
                                corresponding urls to the main concurrentHashMap (imagesUrls)
                             */
                            if(imagesMap.containsKey(tag)){
                                Set<String> urlListing = imagesMap.get(tag);
                                urlListing.addAll(urlSet);
                            }else{
                                imagesMap.putIfAbsent(tag, new HashSet<>());
                            }
                        });

                    }else{
                        System.out.println("[NOT FOUND]:" + topLevelKey + " was not found in concurrent hash map....");
                    }
                    latch.countDown();
                });
            }
            /*
                Calling the await method on latch will block the main thread from running until
                the created tasks are done. Other APIs that have functional similarities to the
                CountDownLock API: Semaphore, Phaser, Future, CompletableFuture
            */
            latch.await();
            pool.shutdown();
        }else{
            System.out.println("[IN PROGRESS]:Starting single threaded crawler....");

            WebGraphADT graphADT = null;
            for(ConcurrentHashMap.Entry<String,ConcurrentHashMap<String,Set<String>>> root:this.imagesUrls.entrySet()){
                graphADT = new WebGraphADT(root.getKey(),root.getValue(),processor,mode);
            }
            if(graphADT != null) graphADT.beginTraversal();
        }
    }

    private void createRootMap(){
        for(String url:this.urlSet){
            if(!this.imagesUrls.containsKey(url)){
                this.imagesUrls.putIfAbsent(url,new ConcurrentHashMap<>());
            }
        }
    }


}
